import numpy as np

# =============================
# ğŸ¯ Objectif : ImplÃ©mentation dâ€™un Mini-XGBoost (un seul arbre, sans rÃ©gularisation avancÃ©e)
# 
# Structure de Mini-XGBoost :
# - compute_gradients : calcule les gradients et hessiennes (MSE)
# - calculate_gain : calcule le gain de split basÃ© sur les gradients
# - find_best_split : identifie la meilleure coupure sur les features
# - TreeNode : structure dâ€™arbre binaire pour les prÃ©dictions
# - build_tree : construit lâ€™arbre rÃ©cursivement avec logique de gain
# - predict_tree / predict : prÃ©dictions rÃ©cursives avec lâ€™arbre
# - train_boosting_tree : entraÃ®ne un arbre sur les rÃ©sidus actuels
# 
# Diagramme d'interaction des fonctions :
# 
#        train_boosting_tree
#                â”‚
#                â–¼
#       compute_gradients
#                â”‚
#                â–¼
#           build_tree
#                â”‚
#                â”œâ”€â”€> find_best_split
#                â”‚         â””â”€â”€> calculate_gain
#                â”‚
#                â””â”€â”€> rÃ©cursion build_tree
# 
#        predict
#           â”‚
#           â””â”€â”€> predict_tree (appel rÃ©cursif)
# =============================

# =============================
# ğŸ§® Ã‰tape 1 : Fonctions de perte et gradients
# =============================
def compute_gradients(y_true, y_pred):
    """
    Calcule les gradients et hessiennes de la fonction de perte quadratique (MSE).

    Loss(y, y_pred) = 1/2 * (y - y_pred)^2
    Gradient = y_pred - y
    Hessienne = 1 (car dÃ©rivÃ©e seconde de la loss quadratique est constante)

    Args:
        y_true (np.array): Les vraies valeurs cibles.
        y_pred (np.array): Les prÃ©dictions courantes du modÃ¨le.

    Returns:
        gradients (np.array): Les gradients pour chaque Ã©chantillon.
        hessians (np.array): Les hessiennes pour chaque Ã©chantillon (ici, vecteur de 1).
    """
    gradients = y_pred - y_true
    hessians = np.ones_like(y_true)
    return gradients, hessians

# =============================
# ğŸŒ³ Ã‰tape 2 : Calcul du gain pour un split donnÃ©
# =============================
def calculate_gain(G_left, H_left, G_right, H_right, lambda_):
    """
    Calcule le gain obtenu en rÃ©alisant un split selon la formule XGBoost (sans rÃ©gularisation L1).

    Gain = 1/2 * [(G_L^2)/(H_L + lambda) + (G_R^2)/(H_R + lambda) - (G^2)/(H + lambda)]

    Args:
        G_left (float): Somme des gradients Ã  gauche.
        H_left (float): Somme des hessiennes Ã  gauche.
        G_right (float): Somme des gradients Ã  droite.
        H_right (float): Somme des hessiennes Ã  droite.
        lambda_ (float): ParamÃ¨tre de rÃ©gularisation L2.

    Returns:
        gain (float): Gain du split considÃ©rÃ©.
    """
    def score(G, H):
        return (G ** 2) / (H + lambda_)

    gain = 0.5 * (score(G_left, H_left) + score(G_right, H_right) -
                  score(G_left + G_right, H_left + H_right))
    return gain

# =============================
# ğŸ” Ã‰tape 3 : Recherche du meilleur split
# =============================
def find_best_split(X, gradients, hessians, lambda_, min_samples_leaf=1):
    """
    Recherche le meilleur split en parcourant toutes les colonnes/features et seuils uniques.

    Args:
        X (np.array): DonnÃ©es d'entrÃ©e.
        gradients (np.array): Gradients des Ã©chantillons.
        hessians (np.array): Hessiennes des Ã©chantillons.
        lambda_ (float): ParamÃ¨tre de rÃ©gularisation.
        min_samples_leaf (int): Nombre minimum d'Ã©chantillons par feuille.

    Returns:
        best_feature (int): Index de la feature pour le meilleur split.
        best_threshold (float): Valeur seuil pour le split.
        best_gain (float): Gain correspondant au meilleur split.
    """
    best_gain = -np.inf
    best_feature = None
    best_threshold = None

    for feature_idx in range(X.shape[1]):
        feature_values = X[:, feature_idx]
        thresholds = np.unique(feature_values)

        for threshold in thresholds:
            left_mask = feature_values <= threshold
            right_mask = ~left_mask

            if left_mask.sum() < min_samples_leaf or right_mask.sum() < min_samples_leaf:
                continue

            G_left, H_left = gradients[left_mask].sum(), hessians[left_mask].sum()
            G_right, H_right = gradients[right_mask].sum(), hessians[right_mask].sum()

            gain = calculate_gain(G_left, H_left, G_right, H_right, lambda_)
            if gain > best_gain:
                best_gain = gain
                best_feature = feature_idx
                best_threshold = threshold

    return best_feature, best_threshold, best_gain

# =============================
# ğŸŒ¿ Ã‰tape 4 : Construction de l'arbre de dÃ©cision
# =============================
class TreeNode:
    """
    Classe reprÃ©sentant un noeud dans un arbre de dÃ©cision.
    """
    def __init__(self, is_leaf=False, value=None, feature=None, threshold=None, left=None, right=None):
        self.is_leaf = is_leaf
        self.value = value
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right

def build_tree(X, gradients, hessians, depth, max_depth, lambda_, gamma):
    """
    Construction rÃ©cursive de l'arbre de dÃ©cision via split selon les gradients/hessiennes.

    Args:
        X (np.array): DonnÃ©es d'entrÃ©e.
        gradients (np.array): Gradients Ã  chaque point.
        hessians (np.array): Hessiennes Ã  chaque point.
        depth (int): Profondeur actuelle.
        max_depth (int): Profondeur maximale de l'arbre.
        lambda_ (float): ParamÃ¨tre de rÃ©gularisation.
        gamma (float): Seuil minimal de gain pour un split.

    Returns:
        TreeNode: Noeud racine de l'arbre construit.
    """
    if depth >= max_depth:
        leaf_value = -gradients.sum() / (hessians.sum() + lambda_)
        return TreeNode(is_leaf=True, value=leaf_value)

    feature, threshold, gain = find_best_split(X, gradients, hessians, lambda_)
    if feature is None or gain < gamma:
        leaf_value = -gradients.sum() / (hessians.sum() + lambda_)
        return TreeNode(is_leaf=True, value=leaf_value)

    left_mask = X[:, feature] <= threshold
    right_mask = ~left_mask

    left = build_tree(X[left_mask], gradients[left_mask], hessians[left_mask],
                      depth + 1, max_depth, lambda_, gamma)
    right = build_tree(X[right_mask], gradients[right_mask], hessians[right_mask],
                       depth + 1, max_depth, lambda_, gamma)

    return TreeNode(is_leaf=False, feature=feature, threshold=threshold,
                    left=left, right=right)

# =============================
# ğŸ”® Ã‰tape 5 : PrÃ©diction via lâ€™arbre
# =============================
def predict_tree(x, node):
    """
    PrÃ©dit une valeur pour un seul Ã©chantillon en parcourant lâ€™arbre rÃ©cursivement.
    """
    if node.is_leaf:
        return node.value
    if x[node.feature] <= node.threshold:
        return predict_tree(x, node.left)
    else:
        return predict_tree(x, node.right)

def predict(X, tree):
    """
    PrÃ©dit pour tous les Ã©chantillons en appelant predict_tree.
    """
    return np.array([predict_tree(x, tree) for x in X])

# =============================
# ğŸš€ Ã‰tape 6 : EntraÃ®nement dâ€™un arbre boosting
# =============================
def train_boosting_tree(X, y, y_pred, max_depth=3, lambda_=1.0, gamma=0.0):
    """
    EntraÃ®ne un arbre de boosting sur les rÃ©sidus (en utilisant les gradients).

    Args:
        X (np.array): DonnÃ©es d'entrÃ©e.
        y (np.array): Valeurs cibles.
        y_pred (np.array): PrÃ©dictions actuelles (initialisÃ©es par la moyenne).
        max_depth (int): Profondeur maximale de l'arbre.
        lambda_ (float): RÃ©gularisation L2.
        gamma (float): Gain minimal pour split.

    Returns:
        TreeNode: Arbre entraÃ®nÃ©.
    """
    gradients, hessians = compute_gradients(y, y_pred)
    tree = build_tree(X, gradients, hessians, depth=0, max_depth=max_depth, lambda_=lambda_, gamma=gamma)
    return tree

# =============================
# âœ… Exemple d'utilisation
# =============================
if __name__ == "__main__":
    # Jeu de donnÃ©es jouet
    X = np.array([[1], [2], [3], [4], [5]])
    y = np.array([1, 2, 1.3, 3.75, 2.25])

    # PrÃ©diction initiale = moyenne
    y_pred = np.full_like(y, y.mean(), dtype=float)

    # EntraÃ®nement dâ€™un arbre
    tree = train_boosting_tree(X, y, y_pred, max_depth=2)

    # Nouvelle prÃ©diction
    new_pred = predict(X, tree)

    # Mise Ã  jour des prÃ©dictions
    y_pred += new_pred

    print("Nouvelles prÃ©dictions :", y_pred)
