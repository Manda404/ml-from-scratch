# ğŸ“˜ ThÃ©orie XGBoost â€” Formules et Optimisation

---

## ğŸ¯ Fonction objectif de XGBoost

Ã€ chaque Ã©tape \( t \), XGBoost cherche Ã  minimiser une fonction objectif composÃ©e de :

- une **fonction de perte** \( l \),
- un **terme de rÃ©gularisation** \( \Omega \).

La fonction totale sâ€™Ã©crit :

$$
\mathcal{L}^{(t)} = \sum_{i=1}^n l\left(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)\right) + \Omega(f_t)
$$

oÃ¹ :
- \( \hat{y}_i^{(t-1)} \) est la prÃ©diction de l'Ã©tape prÃ©cÃ©dente,
- \( f_t(x_i) \) est le nouvel arbre Ã  apprendre,
- \( \Omega(f_t) \) pÃ©nalise la complexitÃ© du modÃ¨le.

---

## ğŸ§± Terme de rÃ©gularisation

Le terme de rÃ©gularisation contrÃ´le le **nombre de feuilles** et les **valeurs attribuÃ©es aux feuilles** :

$$
\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2
$$

avec :
- \( T \) : nombre de feuilles de lâ€™arbre,
- \( w_j \) : score assignÃ© Ã  la feuille \( j \),
- \( \gamma \) : pÃ©nalitÃ© par feuille (favorise les arbres compacts),
- \( \lambda \) : rÃ©gularisation L2 (rÃ©duit les poids extrÃªmes).

---

## ğŸ”§ Approximation par expansion de Taylor

Pour optimiser efficacement, XGBoost applique un **dÃ©veloppement de Taylor d'ordre 2** autour de \( \hat{y}_i^{(t-1)} \) :

$$
\mathcal{L}^{(t)} \approx \sum_{i=1}^n \left[ g_i f_t(x_i) + \frac{1}{2} h_i f_t(x_i)^2 \right] + \Omega(f_t)
$$

avec :
- \( g_i = \frac{\partial l(y_i, \hat{y}_i)}{\partial \hat{y}_i} \) (gradient),
- \( h_i = \frac{\partial^2 l(y_i, \hat{y}_i)}{\partial \hat{y}_i^2} \) (hessienne).

Cette approximation permet de construire les arbres **en utilisant uniquement \( g_i \) et \( h_i \)**.

---

## ğŸŒ³ Construction de lâ€™arbre et gain de split

Pour chaque split candidat, XGBoost calcule le **gain** suivant :

$$
\text{Gain} = \frac{1}{2} \left[
\frac{G_L^2}{H_L + \lambda} +
\frac{G_R^2}{H_R + \lambda} -
\frac{(G_L + G_R)^2}{H_L + H_R + \lambda}
\right] - \gamma
$$

avec :
- \( G_L = \sum g_i \) dans la feuille gauche,
- \( H_L = \sum h_i \) dans la feuille gauche,
- \( G_R = \sum g_i \) dans la feuille droite,
- \( H_R = \sum h_i \) dans la feuille droite.

Le split est acceptÃ© si le **gain est positif** (câ€™est-Ã -dire supÃ©rieur Ã  \( \gamma \)).

---

## ğŸƒ Valeur optimale des feuilles

Une fois lâ€™arbre construit, la **valeur optimale dâ€™une feuille** \( j \) est :

$$
w_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}
$$

oÃ¹ \( I_j \) est lâ€™ensemble des exemples dans la feuille \( j \).

---

## ğŸ” Mise Ã  jour des prÃ©dictions

Ã€ chaque Ã©tape, les prÃ©dictions sont mises Ã  jour par :

$$
\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + \eta \cdot f_t(x_i)
$$

oÃ¹ \( \eta \) est le **taux dâ€™apprentissage** (learning rate).

---

## ğŸ§  Remarques

- XGBoost peut Ãªtre utilisÃ© pour la **rÃ©gression** (MSE) ou la **classification binaire** (log loss).
- Lâ€™approche par second ordre permet une **optimisation rapide et prÃ©cise**.
- La rÃ©gularisation \( \gamma \) et \( \lambda \) joue un rÃ´le crucial pour **contrÃ´ler la complexitÃ©** du modÃ¨le.

---

## ğŸ“ RÃ©fÃ©rences

- Chen & Guestrin (2016), *XGBoost: A Scalable Tree Boosting System*  
  https://arxiv.org/abs/1603.02754  
- Documentation officielle : https://xgboost.readthedocs.io/
