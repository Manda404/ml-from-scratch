# ğŸ“˜ XGBoost : ThÃ©orie et ImplÃ©mentation From Scratch

---

## ğŸ¯ Objectif du cours

Ce cours a pour but de comprendre en profondeur le fonctionnement de lâ€™algorithme XGBoost, depuis les **fondations mathÃ©matiques** jusquâ€™Ã  son **implÃ©mentation complÃ¨te Ã  la main**. Lâ€™objectif est d'en saisir les **concepts clÃ©s**, les **formules d'optimisation**, et de dÃ©velopper une version simplifiÃ©e de XGBoost.

---

## ğŸ“Œ 1. Introduction Ã  XGBoost

**XGBoost (Extreme Gradient Boosting)** est une optimisation du **Gradient Boosting** :
- Plus rapide (grÃ¢ce Ã  la parallÃ©lisation et au calcul distribuÃ©),
- Plus rÃ©gularisÃ© (meilleure gestion du surapprentissage),
- Plus performant sur des tÃ¢ches tabulaires (Kaggle, production).

---

## ğŸ§  2. Concepts clÃ©s

### ğŸ”¸ 2.1 Boosting
Le **boosting** est une mÃ©thode dâ€™**apprentissage itÃ©ratif** :
- On ajoute Ã  chaque itÃ©ration un modÃ¨le (souvent un arbre) pour corriger les erreurs du prÃ©cÃ©dent.
- Chaque modÃ¨le apprend sur les **rÃ©sidus (erreurs)** des prÃ©dictions prÃ©cÃ©dentes.

### ğŸ”¸ 2.2 Gradient Boosting
Au lieu de corriger directement les erreurs, on entraÃ®ne les modÃ¨les sur le **gradient nÃ©gatif** de la fonction de perte. Câ€™est une **descente de gradient fonctionnelle**.

### ğŸ”¸ 2.3 Arbres de dÃ©cision comme learners faibles
XGBoost utilise des **arbres CART** (Classification And Regression Trees) comme modÃ¨les faibles.

---

## ğŸ“ 3. Formulation mathÃ©matique de XGBoost

On cherche Ã  **minimiser** la fonction :

\[
\mathcal{L}^{(t)} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t)
\]

- \( l \) : fonction de perte (ex. MSE ou log loss)
- \( f_t \) : le nouvel arbre Ã  apprendre Ã  l'Ã©tape \( t \)
- \( \Omega(f_t) \) : terme de rÃ©gularisation pour Ã©viter lâ€™overfitting :

\[
\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2
\]

avec :
- \( T \) : nombre de feuilles
- \( w_j \) : score assignÃ© Ã  la feuille \( j \)
- \( \gamma, \lambda \) : paramÃ¨tres de rÃ©gularisation

---

### ğŸ”§ 3.1 Approximation par second ordre (Taylor expansion)

Pour faciliter lâ€™optimisation, on fait une expansion de Taylor Ã  lâ€™ordre 2 autour de \( \hat{y}_i^{(t-1)} \) :

\[
\mathcal{L}^{(t)} \approx \sum_{i=1}^n \left[ g_i f_t(x_i) + \frac{1}{2} h_i f_t(x_i)^2 \right] + \Omega(f_t)
\]

oÃ¹ :
- \( g_i = \frac{\partial l(y_i, \hat{y}_i)}{\partial \hat{y}_i} \) : **gradient**
- \( h_i = \frac{\partial^2 l(y_i, \hat{y}_i)}{\partial \hat{y}_i^2} \) : **hessienne**

---

### ğŸŒ³ 3.2 Construction de l'arbre

Soit un arbre avec \( T \) feuilles. On cherche les poids optimaux \( w_j \) pour chaque feuille :

\[
w_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}
\]

\[
\text{Gain (amÃ©lioration)} = \frac{1}{2} \left[ \frac{(\sum_{i \in I_L} g_i)^2}{\sum_{i \in I_L} h_i + \lambda} + \frac{(\sum_{i \in I_R} g_i)^2}{\sum_{i \in I_R} h_i + \lambda} - \frac{(\sum_{i \in I} g_i)^2}{\sum_{i \in I} h_i + \lambda} \right] - \gamma
\]

- \( I_L, I_R \) : indices des points dans les sous-feuilles gauche/droite
- On **choisit le split qui maximise le gain**.

---

## ğŸ§© 4. Algorithme Ã©tape par Ã©tape

1. Initialiser les prÃ©dictions \( \hat{y}_i^{(0)} \) Ã  une constante (ex : moyenne pour MSE).
2. Pour chaque Ã©tape \( t \in [1, T] \) :
   - Calculer les gradients \( g_i \) et hessiennes \( h_i \)
   - Construire un arbre \( f_t \) qui minimise l'objectif approximÃ©
   - Mettre Ã  jour les prÃ©dictions :  
     \[
     \hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + f_t(x_i)
     \]
3. RÃ©pÃ©ter jusquâ€™Ã  convergence ou max de rounds.

---

## ğŸ› ï¸ 5. Vers lâ€™implÃ©mentation from scratch

Pour lâ€™implÃ©mentation dans `xgboost_scratch.py`, voici les Ã©tapes que nous allons suivre :

### âœ… Ã‰tapes de lâ€™implÃ©mentation
- [ ] DÃ©finir une classe `XGBoostRegressor`
- [ ] ImplÃ©menter la loss (ex: MSE â†’ gradients et hessiennes)
- [ ] CrÃ©er un arbre binaire rÃ©cursif basÃ© sur le gain
- [ ] ImplÃ©menter la prÃ©diction arbre par arbre
- [ ] Ajouter rÃ©gularisation \( \lambda \), \( \gamma \)
- [ ] ImplÃ©menter l'ensemble du boosting
- [ ] Ajouter early stopping (facultatif)

---

## ğŸ“ 6. Remarques

- XGBoost repose sur une approximation mathÃ©matique rigoureuse.
- Sa force vient de son **contrÃ´le fin du surapprentissage**, sa **vitesse** et ses **optimisations bas niveau** (non reproduites ici).
- Le but ici est **lâ€™apprentissage, pas la performance**.

---

## âœï¸ RÃ©fÃ©rences

- [Chen & Guestrin, 2016 - XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)
- [XGBoost documentation officielle](https://xgboost.readthedocs.io/)
- [XGBoost GitHub](https://github.com/dmlc/xgboost)